Requirements for NLP processing systems now typically include the need for extremely large scale processing of data derived either from vast online sources or increasing quantities of digitised archive material. 
Whether the scenario is to answer a particular set of research questions or for commercial text analytics, sources such as Twitter provide significantly more data than can be processed and ingested in anything like reasonable time. Parallel computation is typically used to address this bottleneck. 
This poster presents the lessons learnt from our work which emerged from two serendipitous events. First, the need to support digital humanists in their analysis of the full Hansard data set and second a need for a real case study for Lancaster University's High Performance Cluster using textual rather than numerical data. Large infrastructure activities such as CLARIN~\cite{varadi2008clarin} and DARIAH~\cite{constantopoulos2008preparing} are providing distributed archives for language resources but NLP research teams still face local requirements during experimentation to process and reprocess very large resources through complex pipelines. Some toolkits, e.g. GATE can now run in the cloud~\cite{tablan2013gatecloud} to suport such activities. However, many universities have existing high performance clusters that may be under exploited by language researchers. 

% \dr{Todo: signposting}
