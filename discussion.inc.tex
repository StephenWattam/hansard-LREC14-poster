% \begin{itemize}
%     \item Compare against the price of manually parallelising on a "few" commodity systems--
%         \begin{itemize}
%             \item Dev time
%             \item Execution time
%             \item Relative speed up
%             \item Relative difficulty avoiding job overheads, etc.
%         \end{itemize}
% \end{itemize}

Though parallelisation undoubtedly saved countless hours, the design of the toolchain mandated significant manual intervention, and use of HEC machinery arguably did little to favour existing constraints.

Two weeks was spent developing and testing the HEC deployment of the toolchain.  Of this, a significant portion was spent adapting the toolchain to run on the scheduler without restriction from the shared filesystem.

The problem is embarrassingly parallel, and the design of many HEC facilities is well suited to exploit that.  It is certainly the case that, even if we had manually split the data and run it on (faster) commodity hardware, we would still have incurred significant overhead in doing so.  Assuming development in this case is simpler and takes just a week, we would still have required ten desktop systems to run the tagging jobs in order to achieve the same overall completion time.  

It is worth noting that, had the HEC proven too slow for our purposes, there were still many optimisations that could have been made to the toolchain (for example, each worker will support many instances of the toolchain, or scripts may have been modified to allow pipelining).  Though these incur further development effort, they may be worthwhile for many projects.

In situations where no central scheduler is used (such as manually splitting input across a number of desktop systems), homogeniety of job sizes is of great importance.  Where scheduling is automated, there is a benefit to having smaller jobs, as they may compete more efficiently for resources (though this must be balanced with the overheads of any scheduling itself).

For problems where time constraints are less severe, the toolchain used is particularly specialised, or the data is less inherently parallel, there may be benefits to the simpler approach.  For our purposes, where the task was extremely parallel, the toolchain easily ported, and the timescale short, the greater concurrency offered by a high performance cluster proved invaluable.










% 
% 
% but the HEC incurs severe overheads for some operations that are fast on commodity systems.  This effect is particularly noticable as the toolchain was designed with such systems in mind.  One possible alternative to use of such specialised hardware would be manually parallelising the toolchain over a number of desktop systems.  
% 
% This has a number of advantages in terms of development time, and greatly simplifies the deployment in terms of software requirements (especially as a whole system can then be booted from cloned external disks or the network).  
% 
% 
% In situations where the toolchain to be run is not inherently parallel, and the problem is, this could prove a relative cheap, fast, and easy option.
% 
% \dr{Revise the above when data about relative single-worker speed comes in}
% 
% The structure of the HEC required that jobs were constructed to take a minimum time, in order to minimise the load on I/O systems.  This constraint increases the risk of running wildly heterogenous tasks, something that ultimately reduces the efficiency of the parallelisation as jobs run out.  There are three possible solutions to this: jobs can be made as small as possible whilst testing for I/O stress (this is the approach we used); job duration may be modelled and a batch size chosen to trade off the two properties; or the input data may be re-distributed between to create batches with homogenous runtimes.  The heterogeniety of our runtimes indicates that, for larger corpora where there is are stricter runtime constraints, a mix of these three methods may be necessary to ensure efficient and timely execution.
% 
