The system used for processing was the Lancaster university High-End Computing cluster (HEC).  This consists of a number of compute nodes running Scientific Linux, connected using the Sun Grid Engine.  In total the system comprises over 2200 CPU cores, 11TB of memory, 32TB of high performance (working) file storage, and a further 1PB of medium performance storage.


\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{arch.png}
\caption{HEC architecture}
\label{fig:arch}
\end{figure}

As shown in figure~\ref{fig:arch}, the system is reliant on network-attached shared storage in the form of a Panasas Activescale Series 8 storage cluster.  Intermediate storage is provided by a number of other storage nodes \todo{Do these provide the working shared storage/scratch, or are they only for use with GridPP?}.

There are 262 compute nodes in total, each with two four- or six-core CPUs\todo{Xeons?} each.  Most of these compute nodes have 24GB of RAM available locally (those that do not have 96GB).

Jobs are deployed on the HEC using the Sun Grid Engine scheduling framework, and works in a batch processing manner.  Jobs contend with one another for access to compute nodes.  This scheduler can be used to queue up job arrays in large batches, which will then be distributed to each compute node individually as they become free.  This was the primary mechanism used to distribute jobs across nodes.

