% \subsection{Limitations \& Solutions}

The Wmatrix tag wizard toolchain was originally designed and developed to run on commodity computing hardware in a batch processing fashion.  The toolchain thus respects a number of common limits that apply to desktop computers (e.g. low memory limits) and attempts to exploit other resources that are available in abundance (fast sequential file I/O).
Many of these properties do not transfer to larger clusters, and a number of changes had to be made in order to align the processing stages with the resources available on the HEC system.


\paragraph{File Access}
The Wmatrix toolchain makes extensive use of small output and intermediate files during operation.  This incurs a significant performance overhead when used with filesystems that are typical in high-end distributed systems, which often use large block sizes, carry metadata for versioning and redundancy, or are accessible only over network links.
For this particular process, each input file causes creation of eleven intermediate and output files.  When parallelised, the number of concurrent I/O operations performed, along with the organisation of the corpus (stored as many small files), soon overwhelmed the bandwidth limitations of the shared filesystem.

The solution to this effected changes to the whole toolchain---first, the corpus was restructured to move each day's files into a single \mon{tar} archive.  This could then be copied with a single operation from the shared drive.
This input file was then extracted into a temporary directory on the local disk of the compute node.
Following execution, output was again archived using \mon{tar} and copied back to the shared drive.  This process necessitated a further post--processing stage to extract and validate the output, which was performed after downloading data from the HEC.


\paragraph{Memory Size}
Memory provisions on systems have grown considerably since the toolchain was designed, and as such it fails to exploit even modern desktop systems. The toolchain does this by trading time for space in some instances and by caching data on disk, both of which would have requied significant redevelopment to alter.  We thus elected not to fully exploit each compute node's relatively high 24GB of available memory.




% If we need to further reduce words then this could be thinned out or commented out

\paragraph{Indexing and Co-ordination}
As work was completed away from the shared disk, a system was necessary to co-ordinate compute nodes' access to data.
This was accomplished by using a flatfile, centrally stored and accessed once per batch, that contained a list of input archives and their corresponding output directories.  
The numeric `job ID' environment variable provided by the scheduler was used to compute an offset, which each job used to split the input file by line, selecting a batch of input files for a single job.

The number of input archives tagged per job was chosen to balance scheduling overheads with the need to `smooth out' file access and ensure that any failures did not affect large areas of the corpus.  Ultimately, each job was run using 20 input archives, chosen to take 30 minutes using average input files.



\paragraph{Scheduling Overhead}
The overhead incurred by a shared scheduling system is significant compared to a typical process creation task (exacerbated by the need to copy and extract archives).  This overhead was minimised by tagging multiple directories in a single batch: in practice, running 2230 jobs with a predicted execution time of 30 minutes each, this did not prove to be a limiting factor.
% As mentioned above, scheduling overheads were minimised by tagging multiple directories within a single batch, re-using each compute node many times before copying results back.


\paragraph{Per-node Parallelism}
Each compute node on the HEC is furnished with 8-12 processor cores.  Wmatrix itself does not take advantage of this parallelism, and the decision was made not to introduce the complexity of pipelining into each job dispatch script. 
As such, this remains the largest untapped source of further performance.



