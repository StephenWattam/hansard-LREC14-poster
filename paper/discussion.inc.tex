% \begin{itemize}
%     \item Compare against the price of manually parallelising on a "few" commodity systems--
%         \begin{itemize}
%             \item Dev time
%             \item Execution time
%             \item Relative speed up
%             \item Relative difficulty avoiding job overheads, etc.
%         \end{itemize}
% \end{itemize}

Two weeks was spent developing and testing the HEC deployment of the toolchain.  Of this, a significant portion was spent adapting the toolchain to run on the scheduler without restriction from the shared filesystem.
The problem is embarrassingly parallel, and the design of many HEC facilities is well suited to exploit that.  It is certainly the case that, even if we had manually split the data and run it on (faster) commodity hardware, we would still have incurred significant overhead in doing so.  Assuming development in that case were simpler and took just a week, we would have required ten desktop systems in order to achieve the same overall completion time.

It is worth noting that we did not fully exploit the parallel capacity of the cluster, and modifications such as parallelising upon each compute node, or pipelining, would yield vastly improved execution times.  In part this was a response to the limitations of the HEC's design, particularly disk access rate and the simplicity of the scheduling system.
Though these incur further development effort, they may be worthwhile for many projects, or be used to extend the lifespan of existing toolchains.

In contrast to manual deployment, there is a benefit to having smaller jobs where scheduling is automated and fast, as they may compete more efficiently for resources (though this must be balanced with the overheads of any scheduling itself).
For problems where time constraints are less severe, the toolchain used is particularly specialised, or the data is less inherently parallel, there may be benefits to the simpler approach.  For our purposes, where
% the task was extremely parallel, 
the toolchain was easily ported, and the timescale short, the greater concurrency offered by a high performance cluster proved invaluable.  

% --

\vspace{10pt}

In future work, we will test the HEC facilities with other corpora with different filesize distribution characteristics. We will also experiment with the MapReduce programming model through implementations such as Apache Hadoop. A key future requirement is to expose such parallel processing facilities through a programming-lite interface so that the benefits can be exploited by corpus linguists and digital humanists alike.










% 
% 
% but the HEC incurs severe overheads for some operations that are fast on commodity systems.  This effect is particularly noticable as the toolchain was designed with such systems in mind.  One possible alternative to use of such specialised hardware would be manually parallelising the toolchain over a number of desktop systems.  
% 
% This has a number of advantages in terms of development time, and greatly simplifies the deployment in terms of software requirements (especially as a whole system can then be booted from cloned external disks or the network).  
% 
% 
% In situations where the toolchain to be run is not inherently parallel, and the problem is, this could prove a relative cheap, fast, and easy option.
% 
% \dr{Revise the above when data about relative single-worker speed comes in}
% 
% The structure of the HEC required that jobs were constructed to take a minimum time, in order to minimise the load on I/O systems.  This constraint increases the risk of running wildly heterogenous tasks, something that ultimately reduces the efficiency of the parallelisation as jobs run out.  There are three possible solutions to this: jobs can be made as small as possible whilst testing for I/O stress (this is the approach we used); job duration may be modelled and a batch size chosen to trade off the two properties; or the input data may be re-distributed between to create batches with homogenous runtimes.  The heterogeniety of our runtimes indicates that, for larger corpora where there is are stricter runtime constraints, a mix of these three methods may be necessary to ensure efficient and timely execution.
% 
