% \begin{itemize}
%     \item What needed doing.
%     \item Why this could be interesting to other people.
%     \item Other people's use of CLAWS, similar toolchains, HPCs.
%     \item Suitability to process massive [big data] corpora with conventional toolkits simply by throwing money/hardware at it.
%     \item Signposting.
% \end{itemize}


Requirements for NLP processing systems now typically include the need for extremely large scale processing of data derived either from vast online sources or increasing quantities of digitised archive material. 
Whether the scenario is to answer a particular set of research questions or for commercial text analytics, sources such as Twitter provide significantly more data than can be processed and ingested in anything like reasonable time. Parallel computation is typically used to address this bottleneck. 


This poster presents the lessons learnt from our work which emerged from two serendipitous events. First, the need to support digital humanists in their analysis of the full Hansard data set and second a need for a real case study for Lancaster University's High Performance Cluster using textual rather than numerical data. Large infrastructure activities such as CLARIN~\cite{varadi2008clarin} and DARIAH~\cite{constantopoulos2008preparing} are providing distributed archives for language resources but NLP research teams still face local requirements during experimentation to process and reprocess very large resources through complex pipelines. Some toolkits, e.g. GATE can now run in the cloud~\cite{tablan2013gatecloud} to suport such activities. However, many universities have existing high performance clusters that may be under exploited by language researchers. 

\dr{Todo: signposting}
